{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ãgora - RNNs & LSTMs**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b9NbJkYivg3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is divided in two parts.\n",
        "First one:\n",
        "\n",
        "* Introduction to sequential data and Recurrent Neural Networks (RNNs)\n",
        "* Implementing RNNs using PyTorch\n",
        "* Applications: Text classification and time series prediction.\n",
        "\n",
        "Second one:\n",
        "- Understanding LSTM networks for long-term dependencies.\n",
        "- Implementing LSTM in PyTorch for sequence modeling.\n",
        "- Use cases: Sentiment analysis and language modeling."
      ],
      "metadata": {
        "id": "6QqJdTErwhCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. RNNs implementation**"
      ],
      "metadata": {
        "id": "ekkdnQ2kv64f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we are going to define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, then we'll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters:\n",
        "\n",
        "* **input_size** - the size of the input `(seq_len, batch, input_size)`\n",
        "\n",
        "* **hidden_size** - the number of features in the RNN output and in the hidden state. `(n_layers * num_directions, batch, input_size)` where num_layers is the number of stacked RNNs. `num_directions = 2` for bidirectional RNNs and 1 otherwise.\n",
        "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n",
        "* **batch_first** - whether or not the input/output of the RNN will have the batch_size as the first dimension `(batch_size, seq_length, hidden_size)`\n",
        "\n",
        "In the output:\n",
        "* **out** is the output of the RNN from all timesteps from the last RNN layer. It is of the size `(seq_len, batch, num_directions * hidden_size)`.\n",
        "\n",
        "* **h_n** is the hidden value from the last time-step of all RNN layers. It is of the size `(num_layers * num_directions, batch, hidden_size)`.\n",
        "\n",
        "Take a look at the RNN [documentation](https://pytorch.org/docs/stable/nn.html#rnn) to read more about recurrent layers."
      ],
      "metadata": {
        "id": "qYBVrAoDvg8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by implementing a basic RNN in PyTorch for text classification. Let's use a small text dataset for binary classification."
      ],
      "metadata": {
        "id": "rr_NOQKkxJsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uWjOv9k90Il4"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a small custom dataset for binary text classification, and check one batch tensor to verify its dimensions and padding correctness."
      ],
      "metadata": {
        "id": "Zco3oCvKvhEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = set(' '.join(texts).split())  # Simple word-level vocab\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)} #dictionary mapping each word to a unique integer.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx].split()\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = [self.word2idx[word] for word in text]\n",
        "        return torch.tensor(encoded_text), torch.tensor(label)\n",
        "\n",
        "# Example data\n",
        "texts = [\n",
        "    'The sky is blue',\n",
        "    'The sun is bright',\n",
        "    'Aliens have landed on Earth',\n",
        "    'The moon is made of cheese',\n",
        "    'NASA launches a new satellite',\n",
        "    'Breaking news: The world is flat!',\n",
        "]\n",
        "labels = [0, 0, 1, 1, 0, 1]  # 0 = Real, 1 = Fake\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = TextDataset(texts, labels)\n",
        "\n",
        "# Custom collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True)\n",
        "    return texts_padded, torch.tensor(labels)\n",
        "\n",
        "# Create DataLoader with custom collate function\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Verify batch data (just to test)\n",
        "for inputs, labels in dataloader:\n",
        "    print(f'Input batch shape: {inputs.shape}')\n",
        "    print(f'Label batch: {labels}')\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA9m3E_h0e4R",
        "outputId": "72a11862-57b0-4918-a3de-095d443b122b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([2, 6])\n",
            "Label batch: tensor([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define RNN model"
      ],
      "metadata": {
        "id": "f9yGmRIWyZLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define an RNN model for text classification, distinguising between real and fake news.\n",
        "\n",
        "* **Embedding layer:** turns word indices into dense\n",
        "vectors.\n",
        "* **RNN layer:** processes the sequence and captures temporal patterns.\n",
        "* **Fully connected layer:** classifies based on the final hidden state.\n",
        "* Uses `log_softmax` for output (log probabilities for each class).\n"
      ],
      "metadata": {
        "id": "TH_8QqyeyNIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, hidden = self.rnn(x)\n",
        "        out = self.fc(hidden[-1])  # Use the last hidden state\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(dataset.vocab) #is the number of unique words in your dataset\n",
        "embed_size = 10 #size of each word vector (input size)\n",
        "hidden_size = 20\n",
        "output_size = 2  # Binary classification\n",
        "\n",
        "# Initialize model\n",
        "model = RNNClassifier(vocab_size, embed_size, hidden_size, output_size).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw2cNGHj0e65",
        "outputId": "f1c698fe-77d4-44ac-a309-8155e0388a1a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNClassifier(\n",
            "  (embedding): Embedding(24, 10)\n",
            "  (rnn): RNN(10, 20, batch_first=True)\n",
            "  (fc): Linear(in_features=20, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "K3j0IhA82WOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each batch:\n",
        "* Performs a forward pass to generate predictions.\n",
        "* Computes the loss using a criterion like + negative log likelihood or cross-entropy.\n",
        "* Uses **BPTT (Backpropagation Through Time)** to compute gradients.\n",
        "* Updates model parameters with gradient descent.\n",
        "* Logs the average loss per epoch to monitor convergence."
      ],
      "metadata": {
        "id": "InA60Tkq409o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "epochs = 100  # Number of times the model will see the full training data\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0  # To track cumulative loss per epoch\n",
        "\n",
        "    # Iterate through batches of the DataLoader\n",
        "    for inputs, labels in dataloader:\n",
        "        # Move input and labels to the GPU or CPU as per availability\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        outputs = model(inputs)  # Forward pass through the RNN model\n",
        "        loss = criterion(outputs, labels)  # Compute loss between predictions and ground truth\n",
        "        loss.backward()  # Backpropagate error (Backpropagation Through Time)\n",
        "        optimizer.step()  # Update model parameters using gradients\n",
        "\n",
        "        total_loss += loss.item()  # Accumulate batch loss\n",
        "        losses.append(total_loss / len(dataloader))\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB6EA9d70e9e",
        "outputId": "844d2f6d-67e8-447a-e9a4-a3cb05a7776b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.8714\n",
            "Epoch [2/100], Loss: 0.6872\n",
            "Epoch [3/100], Loss: 0.8098\n",
            "Epoch [4/100], Loss: 0.7540\n",
            "Epoch [5/100], Loss: 0.7339\n",
            "Epoch [6/100], Loss: 0.6482\n",
            "Epoch [7/100], Loss: 0.7041\n",
            "Epoch [8/100], Loss: 0.7179\n",
            "Epoch [9/100], Loss: 0.6612\n",
            "Epoch [10/100], Loss: 0.6681\n",
            "Epoch [11/100], Loss: 0.6458\n",
            "Epoch [12/100], Loss: 0.6306\n",
            "Epoch [13/100], Loss: 0.6328\n",
            "Epoch [14/100], Loss: 0.6307\n",
            "Epoch [15/100], Loss: 0.6096\n",
            "Epoch [16/100], Loss: 0.5998\n",
            "Epoch [17/100], Loss: 0.5876\n",
            "Epoch [18/100], Loss: 0.5550\n",
            "Epoch [19/100], Loss: 0.5504\n",
            "Epoch [20/100], Loss: 0.5575\n",
            "Epoch [21/100], Loss: 0.5257\n",
            "Epoch [22/100], Loss: 0.5283\n",
            "Epoch [23/100], Loss: 0.5048\n",
            "Epoch [24/100], Loss: 0.4834\n",
            "Epoch [25/100], Loss: 0.5153\n",
            "Epoch [26/100], Loss: 0.4553\n",
            "Epoch [27/100], Loss: 0.4494\n",
            "Epoch [28/100], Loss: 0.4713\n",
            "Epoch [29/100], Loss: 0.4166\n",
            "Epoch [30/100], Loss: 0.4352\n",
            "Epoch [31/100], Loss: 0.4188\n",
            "Epoch [32/100], Loss: 0.3679\n",
            "Epoch [33/100], Loss: 0.3542\n",
            "Epoch [34/100], Loss: 0.4082\n",
            "Epoch [35/100], Loss: 0.3670\n",
            "Epoch [36/100], Loss: 0.3499\n",
            "Epoch [37/100], Loss: 0.2946\n",
            "Epoch [38/100], Loss: 0.3709\n",
            "Epoch [39/100], Loss: 0.2866\n",
            "Epoch [40/100], Loss: 0.2727\n",
            "Epoch [41/100], Loss: 0.2583\n",
            "Epoch [42/100], Loss: 0.2368\n",
            "Epoch [43/100], Loss: 0.2238\n",
            "Epoch [44/100], Loss: 0.2364\n",
            "Epoch [45/100], Loss: 0.2029\n",
            "Epoch [46/100], Loss: 0.1788\n",
            "Epoch [47/100], Loss: 0.1739\n",
            "Epoch [48/100], Loss: 0.1665\n",
            "Epoch [49/100], Loss: 0.1738\n",
            "Epoch [50/100], Loss: 0.1371\n",
            "Epoch [51/100], Loss: 0.1326\n",
            "Epoch [52/100], Loss: 0.1423\n",
            "Epoch [53/100], Loss: 0.1328\n",
            "Epoch [54/100], Loss: 0.1038\n",
            "Epoch [55/100], Loss: 0.1730\n",
            "Epoch [56/100], Loss: 0.0942\n",
            "Epoch [57/100], Loss: 0.0894\n",
            "Epoch [58/100], Loss: 0.0789\n",
            "Epoch [59/100], Loss: 0.0740\n",
            "Epoch [60/100], Loss: 0.0728\n",
            "Epoch [61/100], Loss: 0.1146\n",
            "Epoch [62/100], Loss: 0.0615\n",
            "Epoch [63/100], Loss: 0.0577\n",
            "Epoch [64/100], Loss: 0.0548\n",
            "Epoch [65/100], Loss: 0.0619\n",
            "Epoch [66/100], Loss: 0.0903\n",
            "Epoch [67/100], Loss: 0.0551\n",
            "Epoch [68/100], Loss: 0.0758\n",
            "Epoch [69/100], Loss: 0.0438\n",
            "Epoch [70/100], Loss: 0.0415\n",
            "Epoch [71/100], Loss: 0.0648\n",
            "Epoch [72/100], Loss: 0.0375\n",
            "Epoch [73/100], Loss: 0.0342\n",
            "Epoch [74/100], Loss: 0.0383\n",
            "Epoch [75/100], Loss: 0.0366\n",
            "Epoch [76/100], Loss: 0.0297\n",
            "Epoch [77/100], Loss: 0.0333\n",
            "Epoch [78/100], Loss: 0.0272\n",
            "Epoch [79/100], Loss: 0.0275\n",
            "Epoch [80/100], Loss: 0.0252\n",
            "Epoch [81/100], Loss: 0.0427\n",
            "Epoch [82/100], Loss: 0.0233\n",
            "Epoch [83/100], Loss: 0.0264\n",
            "Epoch [84/100], Loss: 0.0217\n",
            "Epoch [85/100], Loss: 0.0209\n",
            "Epoch [86/100], Loss: 0.0203\n",
            "Epoch [87/100], Loss: 0.0349\n",
            "Epoch [88/100], Loss: 0.0200\n",
            "Epoch [89/100], Loss: 0.0185\n",
            "Epoch [90/100], Loss: 0.0313\n",
            "Epoch [91/100], Loss: 0.0297\n",
            "Epoch [92/100], Loss: 0.0284\n",
            "Epoch [93/100], Loss: 0.0172\n",
            "Epoch [94/100], Loss: 0.0166\n",
            "Epoch [95/100], Loss: 0.0162\n",
            "Epoch [96/100], Loss: 0.0175\n",
            "Epoch [97/100], Loss: 0.0170\n",
            "Epoch [98/100], Loss: 0.0221\n",
            "Epoch [99/100], Loss: 0.0223\n",
            "Epoch [100/100], Loss: 0.0140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Prediction"
      ],
      "metadata": {
        "id": "27eNjcmT2bdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's test the model on new text data to see whether it can correctly predict if a given statement is real or fake. The model will tokenize and encode the input text, process it through the network, and output a prediction based on the highest probability."
      ],
      "metadata": {
        "id": "zJkK7dC22iaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(model, sentence, word2idx):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        words = sentence.split()\n",
        "        encoded = [word2idx[word] for word in words if word in word2idx]\n",
        "\n",
        "        if not encoded:\n",
        "            return \"Unknown input\"\n",
        "\n",
        "        input_tensor = torch.tensor(encoded).unsqueeze(0).to(device)  # shape: (1, seq_len)\n",
        "        output = model(input_tensor)\n",
        "        pred_label = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        return \"Fake\" if pred_label == 1 else \"Real\"\n",
        "\n",
        "# Example usage\n",
        "test_sentences = [\n",
        "    \"Aliens invade Earth again\",\n",
        "    \"NASA discovers a new planet\",\n",
        "    \"The world is flat according to new study\",\n",
        "    \"Bright sun shines over the mountains\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prediction = predict_text(model, sentence, dataset.word2idx)\n",
        "    print(f\"'{sentence}' => {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjndea5y0fAA",
        "outputId": "06bb483c-ee5b-45fd-b387-68f40473073e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Aliens invade Earth again' => Fake\n",
            "'NASA discovers a new planet' => Real\n",
            "'The world is flat according to new study' => Fake\n",
            "'Bright sun shines over the mountains' => Real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Long Short-Term Memory (LSTM)**"
      ],
      "metadata": {
        "id": "qeYr1u1a4WVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement an LSTM network in PyTorch for a text classification task (sentiment analysis).\n",
        "\n",
        "We will follow the same steps as in the previous section, first load the dataset on movie reviews (0= negative, 1= positive), and then, define the model."
      ],
      "metadata": {
        "id": "oDX-ygPv8Lsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = set(' '.join(texts).split())  # Simple word-level vocab\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx].split()\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = [self.word2idx[word] for word in text]\n",
        "        return torch.tensor(encoded_text), torch.tensor(label)\n",
        "\n",
        "# Example movie reviews\n",
        "texts = [\n",
        "    'This movie is fantastic',\n",
        "    'I hated this movie',\n",
        "    'What a great film',\n",
        "    'Terrible acting and boring plot',\n",
        "    'Best movie I have seen this year',\n",
        "    'Waste of time',\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = TextDataset(texts, labels)\n",
        "\n",
        "# Custom collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True)\n",
        "    return texts_padded, torch.tensor(labels)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "435P23by0fFG"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        out = self.fc(hidden[-1])  # Use the last hidden state\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(dataset.vocab)\n",
        "embed_size = 10\n",
        "hidden_size = 20\n",
        "output_size = 2  # Binary classification (Positive/Negative)\n",
        "\n",
        "# Initialize the model\n",
        "model = LSTMClassifier(vocab_size, embed_size, hidden_size, output_size).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGaYRj5_4efF",
        "outputId": "d2361166-2cbb-44c4-ed90-04a081878965"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMClassifier(\n",
            "  (embedding): Embedding(23, 10)\n",
            "  (lstm): LSTM(10, 20, batch_first=True)\n",
            "  (fc): Linear(in_features=20, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "lFu2j5c3-LT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZI47Rxy4ehe",
        "outputId": "44597db8-20e4-4649-e9b2-9c0650230a8c"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.7091\n",
            "Epoch [2/100], Loss: 0.6761\n",
            "Epoch [3/100], Loss: 0.6550\n",
            "Epoch [4/100], Loss: 0.6733\n",
            "Epoch [5/100], Loss: 0.6494\n",
            "Epoch [6/100], Loss: 0.6970\n",
            "Epoch [7/100], Loss: 0.6904\n",
            "Epoch [8/100], Loss: 0.6892\n",
            "Epoch [9/100], Loss: 0.6577\n",
            "Epoch [10/100], Loss: 0.6614\n",
            "Epoch [11/100], Loss: 0.6471\n",
            "Epoch [12/100], Loss: 0.6517\n",
            "Epoch [13/100], Loss: 0.6256\n",
            "Epoch [14/100], Loss: 0.6432\n",
            "Epoch [15/100], Loss: 0.6345\n",
            "Epoch [16/100], Loss: 0.6426\n",
            "Epoch [17/100], Loss: 0.6073\n",
            "Epoch [18/100], Loss: 0.5992\n",
            "Epoch [19/100], Loss: 0.6219\n",
            "Epoch [20/100], Loss: 0.5877\n",
            "Epoch [21/100], Loss: 0.5958\n",
            "Epoch [22/100], Loss: 0.6065\n",
            "Epoch [23/100], Loss: 0.5940\n",
            "Epoch [24/100], Loss: 0.5720\n",
            "Epoch [25/100], Loss: 0.5799\n",
            "Epoch [26/100], Loss: 0.5742\n",
            "Epoch [27/100], Loss: 0.5472\n",
            "Epoch [28/100], Loss: 0.5359\n",
            "Epoch [29/100], Loss: 0.5265\n",
            "Epoch [30/100], Loss: 0.5170\n",
            "Epoch [31/100], Loss: 0.5007\n",
            "Epoch [32/100], Loss: 0.4904\n",
            "Epoch [33/100], Loss: 0.5034\n",
            "Epoch [34/100], Loss: 0.4652\n",
            "Epoch [35/100], Loss: 0.4480\n",
            "Epoch [36/100], Loss: 0.4734\n",
            "Epoch [37/100], Loss: 0.4119\n",
            "Epoch [38/100], Loss: 0.4528\n",
            "Epoch [39/100], Loss: 0.4414\n",
            "Epoch [40/100], Loss: 0.4227\n",
            "Epoch [41/100], Loss: 0.3671\n",
            "Epoch [42/100], Loss: 0.3284\n",
            "Epoch [43/100], Loss: 0.3296\n",
            "Epoch [44/100], Loss: 0.3155\n",
            "Epoch [45/100], Loss: 0.2975\n",
            "Epoch [46/100], Loss: 0.2838\n",
            "Epoch [47/100], Loss: 0.2931\n",
            "Epoch [48/100], Loss: 0.2514\n",
            "Epoch [49/100], Loss: 0.2492\n",
            "Epoch [50/100], Loss: 0.2662\n",
            "Epoch [51/100], Loss: 0.2068\n",
            "Epoch [52/100], Loss: 0.2272\n",
            "Epoch [53/100], Loss: 0.1957\n",
            "Epoch [54/100], Loss: 0.1663\n",
            "Epoch [55/100], Loss: 0.1297\n",
            "Epoch [56/100], Loss: 0.1471\n",
            "Epoch [57/100], Loss: 0.1414\n",
            "Epoch [58/100], Loss: 0.1227\n",
            "Epoch [59/100], Loss: 0.1233\n",
            "Epoch [60/100], Loss: 0.0900\n",
            "Epoch [61/100], Loss: 0.1022\n",
            "Epoch [62/100], Loss: 0.0943\n",
            "Epoch [63/100], Loss: 0.0785\n",
            "Epoch [64/100], Loss: 0.0880\n",
            "Epoch [65/100], Loss: 0.0592\n",
            "Epoch [66/100], Loss: 0.0771\n",
            "Epoch [67/100], Loss: 0.0696\n",
            "Epoch [68/100], Loss: 0.0675\n",
            "Epoch [69/100], Loss: 0.0638\n",
            "Epoch [70/100], Loss: 0.0594\n",
            "Epoch [71/100], Loss: 0.0573\n",
            "Epoch [72/100], Loss: 0.0441\n",
            "Epoch [73/100], Loss: 0.0438\n",
            "Epoch [74/100], Loss: 0.0411\n",
            "Epoch [75/100], Loss: 0.0448\n",
            "Epoch [76/100], Loss: 0.0396\n",
            "Epoch [77/100], Loss: 0.0376\n",
            "Epoch [78/100], Loss: 0.0386\n",
            "Epoch [79/100], Loss: 0.0278\n",
            "Epoch [80/100], Loss: 0.0274\n",
            "Epoch [81/100], Loss: 0.0234\n",
            "Epoch [82/100], Loss: 0.0314\n",
            "Epoch [83/100], Loss: 0.0315\n",
            "Epoch [84/100], Loss: 0.0300\n",
            "Epoch [85/100], Loss: 0.0288\n",
            "Epoch [86/100], Loss: 0.0274\n",
            "Epoch [87/100], Loss: 0.0253\n",
            "Epoch [88/100], Loss: 0.0189\n",
            "Epoch [89/100], Loss: 0.0199\n",
            "Epoch [90/100], Loss: 0.0212\n",
            "Epoch [91/100], Loss: 0.0167\n",
            "Epoch [92/100], Loss: 0.0210\n",
            "Epoch [93/100], Loss: 0.0153\n",
            "Epoch [94/100], Loss: 0.0138\n",
            "Epoch [95/100], Loss: 0.0143\n",
            "Epoch [96/100], Loss: 0.0171\n",
            "Epoch [97/100], Loss: 0.0181\n",
            "Epoch [98/100], Loss: 0.0128\n",
            "Epoch [99/100], Loss: 0.0153\n",
            "Epoch [100/100], Loss: 0.0169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of the model"
      ],
      "metadata": {
        "id": "H9dj6LmzY8Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to evaluate the performance of the model for two different tasks:\n",
        "1. Prediction of the next word, taking into account the sentence context.\n",
        "2. Sentiment analysis of sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "GB9aviFnZATs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Language modeling (next word prediction)\n",
        "\n",
        "def predict_next_word(model, input_sequence, vocab, word2idx, idx2word):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([word2idx[word] for word in input_sequence.split()], device=device).unsqueeze(0)\n",
        "        output = model(input_tensor)\n",
        "        predicted_idx = torch.argmax(output, dim=1).item()\n",
        "        return idx2word[predicted_idx]\n",
        "\n",
        "# Inverse mapping for word2idx\n",
        "idx2word = {idx: word for word, idx in dataset.word2idx.items()}\n",
        "\n",
        "# Test next word prediction\n",
        "input_seq = 'Best movie I have'\n",
        "predicted_word = predict_next_word(model, input_seq, dataset.vocab, dataset.word2idx, idx2word)\n",
        "print(f'Next word prediction: {predicted_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlaA6FER4ej5",
        "outputId": "0f4adb16-bb12-42f9-c891-d52f1149c799"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word prediction: seen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, text, word2idx):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        words = text.split()\n",
        "        encoded = [word2idx[word] for word in words if word in word2idx]\n",
        "        if not encoded:\n",
        "            return \"Unknown input\"\n",
        "\n",
        "        input_tensor = torch.tensor(encoded).unsqueeze(0).to(device)  # (1, seq_len)\n",
        "        output = model(input_tensor)\n",
        "        predicted_label = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        return \"Positive\" if predicted_label == 1 else \"Negative\"\n",
        "\n",
        "test_sentences = [\n",
        "    \"Amazing performance and great story\",\n",
        "    \"I regret watching this movie\",\n",
        "    \"Not bad, could be better\",\n",
        "    \"Horrible direction and poor acting\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    sentiment = predict_sentiment(model, sentence, dataset.word2idx)\n",
        "    print(f\"'{sentence}' => {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tquEfP10_jSs",
        "outputId": "326c2f7f-7878-4977-baf0-130d4c4e53f1"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Amazing performance and great story' => Positive\n",
            "'I regret watching this movie' => Negative\n",
            "'Not bad, could be better' => Unknown input\n",
            "'Horrible direction and poor acting' => Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC_dTCguFKHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}